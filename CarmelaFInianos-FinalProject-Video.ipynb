{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0788e6b7",
   "metadata": {
    "id": "0788e6b7"
   },
   "source": [
    "# Road Lane Detection\n",
    "## EEN 588 Final Project\n",
    "### Carmela Finianos - 20191424\n",
    "#### May 9, 2023"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8857246b",
   "metadata": {
    "id": "8857246b"
   },
   "source": [
    "Edge detection is a fundamental image processing technique that is used to identify boundaries within an image. The aim of edge detection is to identify the edges of objects within an image, which can then be used for a variety of tasks, such as object recognition, image segmentation, and feature extraction.\n",
    "\n",
    "The process of edge detection involves analyzing the changes in intensity or color values of adjacent pixels in an image. These changes can be detected by applying mathematical operators, such as Sobel, Prewitt, or Canny filters, to the image. These filters highlight areas of high gradient, where the change in intensity between adjacent pixels is significant, and can be used to identify edges.\n",
    "\n",
    "Now, regarding your project, using edge detection for road lane detection is a common application of this technique. By detecting the edges of the road lanes, you can then use this information to help guide an autonomous vehicle or to assist a human driver in staying within the lanes. The process involves capturing a live video feed from a camera mounted on the vehicle, applying edge detection techniques to this feed, and then using the resulting lane boundary information to steer the vehicle.\n",
    "\n",
    "To implement this project, you will need to have a good understanding of computer vision and image processing techniques, as well as knowledge of programming languages such as Python or MATLAB. Additionally, you will need to have access to appropriate hardware, such as a camera and a processor capable of handling the video feed in real-time.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "11506802",
   "metadata": {
    "id": "11506802"
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import os \n",
    "import glob\n",
    "import tensorflow as tf\n",
    "from sklearn.cluster import KMeans\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ce546fd",
   "metadata": {
    "id": "1ce546fd"
   },
   "source": [
    "This code performs shadow detection and correction on an input image. The input image is first converted from BGR color space to YCbCr color space using the cv2.cvtColor() function. A binary mask is created by copying the YCbCr image, and pixels are classified as shadow or non-shadow pixels based on their intensity value. The pixels with intensity values lower than the mean intensity minus one-third of the standard deviation of the Y plane are classified as shadow pixels, and the rest are classified as non-shadow pixels.\n",
    "\n",
    "The misclassified pixels are then removed using morphological operations by performing erosion and dilation on the binary mask using the cv2.erode() function. The sum of pixel intensities in the lit areas and shadow areas and the number of pixels in each area are then calculated. The average pixel intensities in the lit areas and shadow areas are computed, and the difference between them is taken to get the difference in intensity between the lit and shadow areas.\n",
    "\n",
    "The ratio between average shadow pixels and average lit pixels is also calculated. The final step is to add the difference in intensity and ratio to the pixels classified as shadow pixels, using the formula given in the code. Finally, the image is converted back to the BGR color space and the corrected image is returned.\n",
    "\n",
    "Overall, this code performs shadow detection and correction using YCbCr color space and morphological operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7637455c",
   "metadata": {
    "id": "7637455c"
   },
   "outputs": [],
   "source": [
    " def shadow(image):\n",
    "    # covert the BGR image to an YCbCr image\n",
    "    y_cb_cr_img = cv2.cvtColor(image, cv2.COLOR_BGR2YCrCb)\n",
    "\n",
    "    # copy the image to create a binary mask later\n",
    "    binary_mask = np.copy(y_cb_cr_img)\n",
    "\n",
    "    # get mean value of the pixels in Y plane\n",
    "    y_mean = np.mean(cv2.split(y_cb_cr_img)[0])\n",
    "\n",
    "    # get standard deviation of channel in Y plane\n",
    "    y_std = np.std(cv2.split(y_cb_cr_img)[0])\n",
    "\n",
    "    # classify pixels as shadow and non-shadow pixels\n",
    "    for i in range(y_cb_cr_img.shape[0]):\n",
    "      for j in range(y_cb_cr_img.shape[1]):\n",
    "        if y_cb_cr_img[i, j, 0] < y_mean - (y_std / 3):\n",
    "            # paint it white (shadow)\n",
    "            binary_mask[i, j] = [255, 255, 255]\n",
    "        else:\n",
    "            # paint it black (non-shadow)\n",
    "            binary_mask[i, j] = [0, 0, 0]\n",
    "\n",
    "    # Using morphological operation\n",
    "    # The misclassified pixels are\n",
    "    # removed using dilation followed by erosion.\n",
    "    kernel = np.ones((3, 3), np.uint8)\n",
    "    erosion = cv2.erode(binary_mask, kernel, iterations=1)\n",
    "\n",
    "    # sum of pixel intensities in the lit areas\n",
    "    spi_la = 0\n",
    "\n",
    "    # sum of pixel intensities in the shadow\n",
    "    spi_s = 0\n",
    "\n",
    "    # number of pixels in the lit areas\n",
    "    n_la = 0\n",
    "\n",
    "    # number of pixels in the shadow\n",
    "    n_s = 0\n",
    "\n",
    "    # get sum of pixel intensities in the lit areas\n",
    "    # and sum of pixel intensities in the shadow\n",
    "    for i in range(y_cb_cr_img.shape[0]):\n",
    "        for j in range(y_cb_cr_img.shape[1]):\n",
    "            if erosion[i, j, 0] == 0 and erosion[i, j, 1] == 0 and erosion[i, j, 2] == 0:\n",
    "                spi_la = spi_la + y_cb_cr_img[i, j, 0]\n",
    "                n_la += 1\n",
    "            else:\n",
    "                spi_s = spi_s + y_cb_cr_img[i, j, 0]\n",
    "                n_s += 1\n",
    "\n",
    "    # get the average pixel intensities in the lit areas\n",
    "    average_ld = spi_la / n_la\n",
    "\n",
    "    # get the average pixel intensities in the shadow\n",
    "    average_le = spi_s / n_s\n",
    "\n",
    "    # difference of the pixel intensities in the shadow and lit areas\n",
    "    i_diff = average_ld - average_le\n",
    "\n",
    "    # get the ratio between average shadow pixels and average lit pixels\n",
    "    ratio_as_al = average_ld / average_le\n",
    "\n",
    "    # added these difference\n",
    "    for i in range(y_cb_cr_img.shape[0]):\n",
    "        for j in range(y_cb_cr_img.shape[1]):\n",
    "            if erosion[i, j, 0] == 255 and erosion[i, j, 1] == 255 and erosion[i, j, 2] == 255:\n",
    "                y_cb_cr_img[i, j] = [y_cb_cr_img[i, j, 0] + i_diff, y_cb_cr_img[i, j, 1] + ratio_as_al,y_cb_cr_img[i, j, 2] + ratio_as_al]\n",
    "\n",
    "    # covert the YCbCr image to the BGR image\n",
    "    return cv2.cvtColor(y_cb_cr_img, cv2.COLOR_YCR_CB2BGR)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2e45dba",
   "metadata": {
    "id": "b2e45dba"
   },
   "source": [
    "This is a Python function that performs blurring on an input image. Here's what it does:\n",
    "\n",
    "It converts the input image from BGR format to grayscale format using the cv2.cvtColor() function.\n",
    "It applies a Gaussian blur to the grayscale image using the cv2.GaussianBlur() function. The GaussianBlur() function takes three arguments: the image to blur, the size of the kernel (which is set to 5x5), and the standard deviation of the Gaussian kernel (which is set to 0, indicating that it is automatically calculated based on the kernel size).\n",
    "The blurred image is returned by the function using the return statement.\n",
    "Blurring an image can be useful in various computer vision applications, particularly when dealing with edge detection. By reducing high-frequency noise and smoothing out the image, blurring can make it easier to identify edges and other features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29f9cc32",
   "metadata": {
    "id": "29f9cc32"
   },
   "outputs": [],
   "source": [
    "def blurring(image):\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Blur the image for better edge detection\n",
    "    return cv2.GaussianBlur(gray, (5,5), 0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1dae864",
   "metadata": {
    "id": "c1dae864"
   },
   "source": [
    "Canny Edge Detection is a popular image processing algorithm used for edge detection in digital images. It was developed by John F. Canny in 1986.\n",
    "\n",
    "The Canny Edge Detection algorithm involves the following steps:\n",
    "\n",
    "Gaussian filtering: The image is first convolved with a Gaussian filter to reduce the noise in the image.\n",
    "\n",
    "Gradient calculation: The image is then filtered with a Sobel filter to obtain the gradients of the image in the x and y directions.\n",
    "\n",
    "Non-maximum suppression: The gradient magnitudes are then processed to suppress non-maximum pixels. This helps to thin the edges in the image and reduces the chances of detecting false edges.\n",
    "\n",
    "Double thresholding: The image is then thresholded with two different thresholds, a high threshold and a low threshold. The high threshold is used to detect strong edges, while the low threshold is used to detect weak edges.\n",
    "\n",
    "Edge tracking by hysteresis: Finally, the weak edges that are connected to strong edges are also considered as edges.\n",
    "\n",
    "The mathematical formula used for calculating the gradient of an image is:\n",
    "\n",
    "G(x,y) = sqrt(Gx(x,y)^2 + Gy(x,y)^2)\n",
    "\n",
    "where Gx(x,y) and Gy(x,y) are the gradients of the image in the x and y directions, respectively.\n",
    "\n",
    "The non-maximum suppression step involves comparing the gradient magnitude of a pixel with its two neighboring pixels along the gradient direction. If the gradient magnitude of the pixel is not greater than both of its neighboring pixels, then it is set to zero.\n",
    "\n",
    "The double thresholding step involves setting a high threshold and a low threshold. Pixels with gradient magnitudes above the high threshold are considered strong edges, while pixels with gradient magnitudes below the low threshold are considered non-edges. Pixels with gradient magnitudes between the high and low thresholds are considered weak edges.\n",
    "\n",
    "The edge tracking by hysteresis step involves considering the weak edges that are connected to strong edges as edges. This is done by tracing a path along the connected weak edges until a strong edge is found."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35e2af2f",
   "metadata": {
    "id": "35e2af2f"
   },
   "outputs": [],
   "source": [
    "def canny(image):\n",
    "    # Canny Edge Detection\n",
    "    return cv2.Canny(image=image, threshold1=100, threshold2=200) # Canny Edge Detection\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8b0e3e4",
   "metadata": {
    "id": "c8b0e3e4"
   },
   "source": [
    "\n",
    "This code defines a function named \"mask\" that takes two arguments: \"edges\" and \"image\". The purpose of this function is to apply a mask on the edge-detected image (passed as \"edges\" argument), which only keeps the region of interest (ROI) in the image, defined by a polygon.\n",
    "\n",
    "Firstly, a blank mask of the same shape as the \"edges\" image is created using the np.zeros_like() function. The ignore_mask_color is defined as white or (255,255,255) for colored images and 255 for grayscale images. Then, a polygon is defined using the np.array() function. This polygon is used to define the region of interest. The polygon is defined as a list of vertices in the form of (x,y) coordinates, where x and y are the horizontal and vertical coordinates respectively.\n",
    "\n",
    "The polygon is defined to keep only the region of interest in the image. The vertices are defined using the shape of the image passed as the \"image\" argument. The polygon is defined in such a way that it covers the road region in the image, and neglects the sky and other regions. The vertices of the polygon are calculated based on the height and width of the image, and some constants.\n",
    "\n",
    "Finally, the cv2.fillPoly() function is used to fill the pixels inside the polygon with the \"ignore_mask_color\". This creates a mask that has the same shape as the \"edges\" image, but only the pixels inside the polygon have a non-zero value. Then, the cv2.bitwise_and() function is used to apply this mask on the \"edges\" image, which only keeps the ROI and returns the resulting image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f53651c4",
   "metadata": {
    "id": "f53651c4"
   },
   "outputs": [],
   "source": [
    "def mask(edges,image):\n",
    "    #defining a blank mask to start with\n",
    "    mask = np.zeros_like(edges)\n",
    "\n",
    "    #defining a 3 channel or 1 channel color to fill the mask with depending on the input image\n",
    "    if len(image.shape) > 2:\n",
    "        channel_count = img.shape[2]  # i.e. 3 or 4 depending on your image\n",
    "        ignore_mask_color = (255,) * channel_count\n",
    "    else:\n",
    "        ignore_mask_color = 255\n",
    "\n",
    "    imgShape = image.shape\n",
    "    w=imgShape[1]/2\n",
    "    h=imgShape[0]/2\n",
    "    polygon = np.array([ [(w-(w*0.35), imgShape[0]), (w-(w*0.1),h+(h*0.2) ), (w+(w*0.1),h+(h*0.15) ), (w+(w*0.3), imgShape[0]) ] ], dtype=np.int32)\n",
    "    #filling pixels inside the polygon defined by \"vertices\" with the fill color    \n",
    "    cv2.fillPoly(mask, polygon, ignore_mask_color)\n",
    "\n",
    "    #returning the image only where mask pixels are nonzero\n",
    "    return cv2.bitwise_and(edges, mask)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbc563f9",
   "metadata": {
    "id": "fbc563f9"
   },
   "source": [
    "The Hough Transform for lines takes an edge-detected image as input and outputs a set of lines that correspond to the edges in the image. It works by representing each edge point in the image as a line in the Hough space, which is a parameter space that defines all possible lines that could pass through a given edge point. Each line in the Hough space is represented by two parameters, which are typically the distance from the origin to the line and the angle between the line and the x-axis.\n",
    "\n",
    "The Hough Transform algorithm for lines can be summarized as follows:\n",
    "\n",
    "For each edge pixel in the input image, compute a set of lines that could pass through that pixel, represented as (ρ, θ) pairs in Hough space. The (ρ, θ) pairs correspond to the polar coordinates of the line that passes through the edge pixel.\n",
    "\n",
    "Accumulate the (ρ, θ) pairs in a two-dimensional Hough accumulator array. The accumulator array is initialized to all zeros.\n",
    "\n",
    "Once all the edge pixels have been processed, the peaks in the accumulator array correspond to the lines in the input image.\n",
    "\n",
    "A threshold is applied to the accumulator array to remove noise, and the remaining peaks are used to determine the lines in the image.\n",
    "\n",
    "The lines are then extracted from the accumulator array by converting the (ρ, θ) pairs back into Cartesian coordinates.\n",
    "\n",
    "The Hough Transform for lines has several important mathematical properties. One of the key insights is that a line in Cartesian space can be represented as a single point in Hough space, and vice versa. This makes it possible to detect lines in images by searching for clusters of points in Hough space.\n",
    "\n",
    "Another important mathematical property is the parameterization of lines in polar coordinates. This makes it possible to represent lines that have infinite slope (vertical lines) in Cartesian coordinates as points in Hough space, by using the Hessian normal form representation of a line.\n",
    "\n",
    "In summary, the Hough Transform for lines is a powerful technique for detecting edges and lines in an image, and it is based on a parameter space representation that allows for efficient computation and detection of lines in a noisy image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2efbeaea",
   "metadata": {
    "id": "2efbeaea"
   },
   "outputs": [],
   "source": [
    "def hough(roii,image):\n",
    "    linesP = cv2.HoughLinesP(roii, 1, np.pi / 180,20, None, 15, 100)\n",
    "    # draw Hough lines\n",
    "    for line in linesP:\n",
    "        x1, y1, x2, y2 = line[0]\n",
    "        a=(y2-y1)/(x2-x1)\n",
    "        t=math.degrees(math.atan(a))\n",
    "        if abs(a)>(0.5) and abs(t)>39 :\n",
    "          cv2.line(image, (x1, y1), (x2, y2), (255, 0, 0), 3)\n",
    "          \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "277f9064",
   "metadata": {
    "id": "277f9064"
   },
   "outputs": [],
   "source": [
    "data = [] \n",
    "hough=[]\n",
    "I=[]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cda0bb4",
   "metadata": {
    "id": "8cda0bb4"
   },
   "source": [
    "This code block imports images of roads from a specified directory img_dir and loads them into the data list. The images are identified using the file extension *g which will match all files with a .jpg, .jpeg, or .png extension.\n",
    "\n",
    "The os.path.join() function is used to combine the directory path with the file extension pattern to create a full path to all the image files.\n",
    "\n",
    "The glob.glob() function is then used to get a list of all files that match the full path pattern. The resulting list of files is iterated over using a for loop, and each image file is read using the OpenCV cv2.imread() function and added to the data list.\n",
    "\n",
    "The code block also includes a condition to break out of the for loop once 30 images have been loaded into the data list. This is useful if you only want to load a certain number of images for testing or experimentation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "807a309a",
   "metadata": {
    "id": "f8708a68"
   },
   "source": [
    "Video used:https://www.youtube.com/watch?v=6q5_A5wOwDM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53b0a923",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "53b0a923",
    "outputId": "f15ef2ad-7877-4987-d182-a21a2f3a3a91",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(r'C:\\Users\\Carmela\\Downloads\\videoplayback.mp4')\n",
    "while(cap.isOpened()):\n",
    "      \n",
    "# Capture frame-by-frame\n",
    "    ret, frame = cap.read()\n",
    "     # covert the BGR image to an YCbCr image\n",
    "    y_cb_cr_img = cv2.cvtColor(frame, cv2.COLOR_BGR2YCrCb)\n",
    "\n",
    "    # copy the image to create a binary mask later\n",
    "    binary_mask = np.copy(y_cb_cr_img)\n",
    "\n",
    "    # get mean value of the pixels in Y plane\n",
    "    y_mean = np.mean(cv2.split(y_cb_cr_img)[0])\n",
    "\n",
    "  # get standard deviation of channel in Y plane\n",
    "    y_std = np.std(cv2.split(y_cb_cr_img)[0])\n",
    "\n",
    "    # classify pixels as shadow and non-shadow pixels\n",
    "    for i in range(y_cb_cr_img.shape[0]):\n",
    "        for j in range(y_cb_cr_img.shape[1]):\n",
    "            if y_cb_cr_img[i, j, 0] < y_mean - (y_std / 3):\n",
    "                # paint it white (shadow)\n",
    "                binary_mask[i, j] = [255, 255, 255]\n",
    "            else:\n",
    "                # paint it black (non-shadow)\n",
    "                binary_mask[i, j] = [0, 0, 0]\n",
    "\n",
    "    # Using morphological operation\n",
    "    # The misclassified pixels are\n",
    "    # removed using dilation followed by erosion.\n",
    "    kernel = np.ones((3, 3), np.uint8)\n",
    "    erosion = cv2.erode(binary_mask, kernel, iterations=1)\n",
    "\n",
    "    # sum of pixel intensities in the lit areas\n",
    "    spi_la = 0\n",
    "\n",
    "    # sum of pixel intensities in the shadow\n",
    "    spi_s = 0\n",
    "\n",
    "    # number of pixels in the lit areas\n",
    "    n_la = 0\n",
    "\n",
    "    # number of pixels in the shadow\n",
    "    n_s = 0\n",
    "\n",
    "    # get sum of pixel intensities in the lit areas\n",
    "    # and sum of pixel intensities in the shadow\n",
    "    for i in range(y_cb_cr_img.shape[0]):\n",
    "        for j in range(y_cb_cr_img.shape[1]):\n",
    "            if erosion[i, j, 0] == 0 and erosion[i, j, 1] == 0 and erosion[i, j, 2] == 0:\n",
    "                spi_la = spi_la + y_cb_cr_img[i, j, 0]\n",
    "                n_la += 1\n",
    "            else:\n",
    "                spi_s = spi_s + y_cb_cr_img[i, j, 0]\n",
    "                n_s += 1\n",
    "\n",
    "    # get the average pixel intensities in the lit areas\n",
    "    average_ld = spi_la / n_la\n",
    "\n",
    "    # get the average pixel intensities in the shadow\n",
    "    average_le = spi_s / n_s\n",
    "\n",
    "    # difference of the pixel intensities in the shadow and lit areas\n",
    "    i_diff = average_ld - average_le\n",
    "\n",
    "    # get the ratio between average shadow pixels and average lit pixels\n",
    "    ratio_as_al = average_ld / average_le\n",
    "\n",
    "    # added these difference\n",
    "    for i in range(y_cb_cr_img.shape[0]):\n",
    "        for j in range(y_cb_cr_img.shape[1]):\n",
    "            if erosion[i, j, 0] == 255 and erosion[i, j, 1] == 255 and erosion[i, j, 2] == 255:\n",
    "\n",
    "                y_cb_cr_img[i, j] = [y_cb_cr_img[i, j, 0] + i_diff, y_cb_cr_img[i, j, 1] + ratio_as_al,\n",
    "                                  y_cb_cr_img[i, j, 2] + ratio_as_al]\n",
    "\n",
    "    # covert the YCbCr image to the BGR image\n",
    "    final_image = cv2.cvtColor(y_cb_cr_img, cv2.COLOR_YCR_CB2BGR)\n",
    "\n",
    "\n",
    "    gray = cv2.cvtColor(final_image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Blur the image for better edge detection\n",
    "    img_blur = cv2.GaussianBlur(gray, (5,5), 0) \n",
    "\n",
    "      # Canny Edge Detection\n",
    "    edges = cv2.Canny(image=img_blur, threshold1=100, threshold2=200) # Canny Edge Detection\n",
    "\n",
    "    #defining a blank mask to start with\n",
    "    mask = np.zeros_like(edges)\n",
    "\n",
    "\n",
    "    #defining a 3 channel or 1 channel color to fill the mask with depending on the input image\n",
    "    if len(frame.shape) > 2:\n",
    "        channel_count = frame.shape[2]  # i.e. 3 or 4 depending on your image\n",
    "        ignore_mask_color = (255,) * channel_count\n",
    "    else:\n",
    "        ignore_mask_color = 255\n",
    "\n",
    "    imgShape = frame.shape\n",
    "    w=imgShape[1]/2\n",
    "    h=imgShape[0]/2\n",
    "    polygon = np.array([ [(w-(w*0.5), imgShape[0]), (w-(w*0.2),h+(h*0.5) ), (w+(w*0.1),h+(h*0.4) ), (w+(w*0.3), imgShape[0]) ] ], dtype=np.int32)\n",
    "    #filling pixels inside the polygon defined by \"vertices\" with the fill color    \n",
    "    cv2.fillPoly(mask, polygon, ignore_mask_color)\n",
    "\n",
    "    #returning the image only where mask pixels are nonzero\n",
    "    roii = cv2.bitwise_and(edges, mask)\n",
    "    plt.imshow(roii)\n",
    "    plt.show()\n",
    "    linesP = cv2.HoughLinesP(roii, 1, np.pi / 180,20, None, 1, 100)\n",
    "      # draw Hough lines\n",
    "    for line in linesP:\n",
    "        x1, y1, x2, y2 = line[0]\n",
    "        a=(y2-y1)/(x2-x1)\n",
    "        j=math.degrees(math.atan(a))\n",
    "        if abs(a)>0.5 and abs(j)>27:\n",
    "            cv2.line(frame, (x1, y1), (x2, y2), (255, 0, 0), 3)\n",
    " \n",
    "    cv2.imshow('Video', frame)\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# When everything is done, release the capture\n",
    "video_capture.release()\n",
    "cv2.destroyAllWindows()\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58b8b6a3",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PHQFh9sXVpjo",
    "outputId": "9ccc8c1e-a435-4509-c9e8-9401c54b406c"
   },
   "source": [
    "\n",
    "After completing the project, it can be concluded that the accuracy achieved was moderate, although it fell short of the initial expectations. The project posed several challenges and required a considerable amount of time and effort to overcome them. Despite the best efforts and utilization of various techniques, the desired level of performance could not be achieved.\n",
    "\n",
    "The project presented several complexities and required in-depth knowledge of computer vision techniques, algorithms, and programming. The process involved various steps, including image preprocessing, edge detection, Hough Transform for line detection, and masking. The implementation of these steps required careful consideration of various parameters and fine-tuning of their values to achieve optimal performance.\n",
    "\n",
    "However, despite the challenges, the project provided valuable insights and learning opportunities into computer vision techniques and their practical applications. The process of identifying and addressing the limitations of the project allowed for a deeper understanding of the underlying concepts and improved problem-solving skills.\n",
    "\n",
    "In conclusion, while the project fell short of the initial expectations, it provided significant learning experiences and insights into computer vision techniques. It highlighted the complexities and challenges involved in implementing such projects and underscored the importance of meticulous planning, careful implementation, and persistence in the face of challenges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0491b4bb",
   "metadata": {
    "id": "0491b4bb"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "195461ed",
   "metadata": {
    "id": "195461ed"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
